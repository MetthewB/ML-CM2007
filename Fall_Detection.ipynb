{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Fall Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:\n",
    "Matthieu Beylard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess label and sensor datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Function to find a task from text\n",
    "def find_task(text):\n",
    "    if pd.isnull(text):\n",
    "        return None\n",
    "    else:\n",
    "        match = re.search(r'\\((\\d+)\\)', str(text))\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "subjects = [f\"{i:02d}\" for i in range(6, 39) if i != 34] # No data for subject 34\n",
    "\n",
    "total_labels_data = []\n",
    "total_sensor_data = []\n",
    "\n",
    "for subject in subjects:\n",
    "    # Preprocess label files (generate DataFrame, extract data, fill NaN values, apply find_task on Task ID)\n",
    "    label_path = \"label_data/SA\" + subject + \"_label.xlsx\"\n",
    "    df = pd.read_excel(label_path, sheet_name = 'Sheet1')\n",
    "\n",
    "    data_label = df.iloc[:, [0, 2, 3, 4]]\n",
    "    data_label = data_label.fillna(method = 'ffill')\n",
    "\n",
    "    data_label.iloc[:, 0] = data_label.iloc[:, 0].apply(find_task)\n",
    "\n",
    "    start = df['Fall_onset_frame']\n",
    "    stop = df['Fall_impact_frame']\n",
    "\n",
    "\n",
    "    # Extract indexes from sensor files\n",
    "    sensor_files = glob.glob('sensor_data/SA' + subject + '/**/*.csv', recursive=True)\n",
    "    sensor_files.sort()\n",
    "\n",
    "    indexes = []\n",
    "    for file in sensor_files:\n",
    "        match = re.search(r'T(\\d{2})R', file)\n",
    "        extracted_index = match.group(1)\n",
    "        indexes.append(int(extracted_index))\n",
    "\n",
    "    count_phase_1 = 0\n",
    "    for idx in indexes:\n",
    "        if idx < 20: # No fall in the first 19 tasks (phase 1)\n",
    "            count_phase_1 += 1\n",
    "\n",
    "    count_phase_2 = count_phase_1\n",
    "    count_total_falls = 0\n",
    "    for idx in indexes:\n",
    "        if (20 <= idx < 35): # Falls in tasks 20 to 34 (phase 2)\n",
    "            count_phase_2 += 1\n",
    "            count_total_falls += 1\n",
    "\n",
    "    count_phase_3 = count_phase_2\n",
    "    for idx in indexes:\n",
    "        if idx >= 35:  # No fall in the last 2 tasks (phase 3)\n",
    "            count_phase_3 += 1\n",
    "\n",
    "\n",
    "    # Extract data for different types of tasks\n",
    "    phase_1 = []\n",
    "    for i in range(0, count_phase_1):\n",
    "        df = pd.read_csv(sensor_files[i])\n",
    "        frame = df['FrameCounter'].to_numpy()\n",
    "        phase_1.extend(frame)\n",
    "\n",
    "    phase_2 = []\n",
    "    for i in range(count_phase_1, count_phase_2):\n",
    "        df = pd.read_csv(sensor_files[i])\n",
    "        frame = df['FrameCounter'].to_numpy()\n",
    "        phase_2.append(frame)\n",
    "\n",
    "    phase_3 = []\n",
    "    for i in range(count_phase_2, count_phase_3):\n",
    "        df = pd.read_csv(sensor_files[i])\n",
    "        frame = df['FrameCounter'].to_numpy()\n",
    "        phase_3.extend(frame)  \n",
    "\n",
    "\n",
    "    # Generate labels for no fall and falls periods and concatenate labels\n",
    "    labels_phase_1 = np.zeros(len(phase_1))\n",
    "    labels_phase_3 = np.zeros(len(phase_3))\n",
    "\n",
    "    labels_phase_2 = []\n",
    "    for j in range(count_total_falls):\n",
    "        labels = np.zeros(phase_2[j].size)\n",
    "        for k in range(start[j]-1, stop[j]):\n",
    "            labels[k] = 1\n",
    "        labels_phase_2 = np.concatenate((labels_phase_2, labels))\n",
    "\n",
    "    labels_data = np.concatenate((labels_phase_1,labels_phase_2,labels_phase_3))\n",
    "\n",
    "\n",
    "    # Concatenate all CSV files into one DataFrame and save to CSV\n",
    "    merged_sensor_df = pd.DataFrame()\n",
    "\n",
    "    for file in sensor_files:\n",
    "        df = pd.read_csv(file)\n",
    "        merged_sensor_df = pd.concat([merged_sensor_df, df], ignore_index=True)\n",
    "\n",
    "    merged_sensor_df.to_csv ('sensor_dataset.csv', index=False)\n",
    "\n",
    "    sensor_data = merged_sensor_df.to_numpy()\n",
    "\n",
    "\n",
    "    # Extend total labels data and sensor data lists\n",
    "    total_labels_data.extend(labels_data)\n",
    "    total_sensor_data.extend(sensor_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "66918/66918 [==============================] - 131s 2ms/step - loss: 0.0913 - accuracy: 0.9689 - val_loss: 0.0788 - val_accuracy: 0.9721\n",
      "Epoch 2/20\n",
      "66918/66918 [==============================] - 133s 2ms/step - loss: 0.0848 - accuracy: 0.9707 - val_loss: 0.0771 - val_accuracy: 0.9727\n",
      "Epoch 3/20\n",
      "66918/66918 [==============================] - 119s 2ms/step - loss: 0.0828 - accuracy: 0.9711 - val_loss: 0.0753 - val_accuracy: 0.9732\n",
      "Epoch 4/20\n",
      "66918/66918 [==============================] - 121s 2ms/step - loss: 0.0809 - accuracy: 0.9717 - val_loss: 0.0736 - val_accuracy: 0.9735\n",
      "Epoch 5/20\n",
      "66918/66918 [==============================] - 128s 2ms/step - loss: 0.0794 - accuracy: 0.9720 - val_loss: 0.0712 - val_accuracy: 0.9739\n",
      "Epoch 6/20\n",
      "66918/66918 [==============================] - 140s 2ms/step - loss: 0.0786 - accuracy: 0.9722 - val_loss: 0.0709 - val_accuracy: 0.9741\n",
      "Epoch 7/20\n",
      "66918/66918 [==============================] - 139s 2ms/step - loss: 0.0781 - accuracy: 0.9724 - val_loss: 0.0706 - val_accuracy: 0.9740\n",
      "Epoch 8/20\n",
      "66918/66918 [==============================] - 135s 2ms/step - loss: 0.0773 - accuracy: 0.9726 - val_loss: 0.0702 - val_accuracy: 0.9742\n",
      "Epoch 9/20\n",
      "66918/66918 [==============================] - 129s 2ms/step - loss: 0.0769 - accuracy: 0.9728 - val_loss: 0.0700 - val_accuracy: 0.9741\n",
      "Epoch 10/20\n",
      "66918/66918 [==============================] - 138s 2ms/step - loss: 0.0765 - accuracy: 0.9729 - val_loss: 0.0677 - val_accuracy: 0.9745\n",
      "Epoch 11/20\n",
      "66918/66918 [==============================] - 134s 2ms/step - loss: 0.0754 - accuracy: 0.9732 - val_loss: 0.0691 - val_accuracy: 0.9742\n",
      "Epoch 12/20\n",
      "66918/66918 [==============================] - 127s 2ms/step - loss: 0.0746 - accuracy: 0.9734 - val_loss: 0.0666 - val_accuracy: 0.9750\n",
      "Epoch 13/20\n",
      "66918/66918 [==============================] - 138s 2ms/step - loss: 0.0741 - accuracy: 0.9735 - val_loss: 0.0661 - val_accuracy: 0.9754\n",
      "Epoch 14/20\n",
      "66918/66918 [==============================] - 136s 2ms/step - loss: 0.0737 - accuracy: 0.9736 - val_loss: 0.0674 - val_accuracy: 0.9748\n",
      "Epoch 15/20\n",
      "66918/66918 [==============================] - 137s 2ms/step - loss: 0.0735 - accuracy: 0.9737 - val_loss: 0.0677 - val_accuracy: 0.9747\n",
      "Epoch 16/20\n",
      "66918/66918 [==============================] - 134s 2ms/step - loss: 0.0728 - accuracy: 0.9739 - val_loss: 0.0641 - val_accuracy: 0.9759\n",
      "Epoch 17/20\n",
      "66918/66918 [==============================] - 131s 2ms/step - loss: 0.0729 - accuracy: 0.9739 - val_loss: 0.0671 - val_accuracy: 0.9755\n",
      "Epoch 18/20\n",
      "66918/66918 [==============================] - 130s 2ms/step - loss: 0.0725 - accuracy: 0.9742 - val_loss: 0.0649 - val_accuracy: 0.9757\n",
      "Epoch 19/20\n",
      "66918/66918 [==============================] - 133s 2ms/step - loss: 0.0721 - accuracy: 0.9742 - val_loss: 0.0639 - val_accuracy: 0.9762\n",
      "Epoch 20/20\n",
      "66918/66918 [==============================] - 136s 2ms/step - loss: 0.0721 - accuracy: 0.9741 - val_loss: 0.0654 - val_accuracy: 0.9757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2ddde2750>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the label data and sensor data lists into numpy arrays\n",
    "total_labels_data = np.array(total_labels_data)\n",
    "total_sensor_data = np.array(total_sensor_data)\n",
    "\n",
    "# Train-Test split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(total_sensor_data, total_labels_data, test_size=0.33, random_state=1)\n",
    "\n",
    "# Reshape X_train for the LSTM model\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Design the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# LSTM layer (return_sequences=True captures temporal dependencies)\n",
    "model.add(LSTM(units=64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Another LSTM layer\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# LSTM layer (no return_sequences for a single output)\n",
    "model.add(LSTM(units=64))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Dense layer for final classification\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train, epochs=20, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the neural network was designed using three LSTM layers, a dense layer, 20 epochs and a batch size of 32. \n",
    "\n",
    "The LSTM layers are specialized RNN layers that focus on capturing temporal dependencies and learning sequential patterns, while the dense layer are feedforward NN layers where each neuron is connected to every neuron in the previous layer, to make the final classification decision based on the features learned by the LSTM layers.\n",
    "\n",
    "The number of epochs determines how many times the model will iterate over the entire training dataset. I chose to put 20 epochs to ensure the model adapted nicely. One could even make more iterations for as long as the loss and accuracy values get better, these values worsening indicating overfitting.\n",
    "\n",
    "The batch size determines how many samples are propagated through the network before the weights are updated. We want to keep small batch sizes to have a fast convergence, but not too low to avoid noisy updates. \n",
    "\n",
    "Dropout is a technique used to prevent overfitting and improve generalization by randomly dropping a proportion of neurons (20%) from the network during each training epoch. Stopping methods also prevent overfitting and improve generalization, for example by using the EarlyStop() method. Optimizers update the weights of the neural network based on the loss function and the gradients of the parameters. We used Adam, because of its ability to use RMS propagation, correct bias and adapt the learning rate for each parameter individually, making it quite robust."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CM2007",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
